{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfDcVFPtg9L8"
      },
      "outputs": [],
      "source": [
        "#base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "weightff = []\n",
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "\n",
        "        self.bias -= learning_rate * output_error\n",
        "\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "FWY8Mxrkk8QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error"
      ],
      "metadata": {
        "id": "O0HqMoczlIfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function and its derivative\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-np.tanh(x)**2;"
      ],
      "metadata": {
        "id": "0YfCjcgTlLrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function and its derivative\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "PT5qvHVglOd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            w = []\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagations\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n"
      ],
      "metadata": {
        "id": "QXMpSCm6lSvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Network\n",
        "net = Network()\n",
        "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(50, 10))                   # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# train on 1000 samples\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use(mse, mse_prime)\n",
        "\n",
        "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
        "\n",
        "# test on 3 samples\n",
        "testnum = 100\n",
        "out = net.predict(x_test[0:testnum])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"predicted values : \")\n",
        "print(out, end=\"\\n\")\n",
        "print(\"true values : \")\n",
        "print(y_test[0:testnum])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w2M9R02lZw1",
        "outputId": "9c2806df-0e84-442e-acaf-6c9997cef809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "epoch 1/35   error=0.243210\n",
            "epoch 2/35   error=0.102562\n",
            "epoch 3/35   error=0.078610\n",
            "epoch 4/35   error=0.065609\n",
            "epoch 5/35   error=0.056104\n",
            "epoch 6/35   error=0.048710\n",
            "epoch 7/35   error=0.042586\n",
            "epoch 8/35   error=0.037455\n",
            "epoch 9/35   error=0.033283\n",
            "epoch 10/35   error=0.029855\n",
            "epoch 11/35   error=0.026957\n",
            "epoch 12/35   error=0.024490\n",
            "epoch 13/35   error=0.022492\n",
            "epoch 14/35   error=0.020758\n",
            "epoch 15/35   error=0.019286\n",
            "epoch 16/35   error=0.017903\n",
            "epoch 17/35   error=0.016788\n",
            "epoch 18/35   error=0.015810\n",
            "epoch 19/35   error=0.014988\n",
            "epoch 20/35   error=0.014189\n",
            "epoch 21/35   error=0.013534\n",
            "epoch 22/35   error=0.012962\n",
            "epoch 23/35   error=0.012516\n",
            "epoch 24/35   error=0.012031\n",
            "epoch 25/35   error=0.011634\n",
            "epoch 26/35   error=0.011117\n",
            "epoch 27/35   error=0.010623\n",
            "epoch 28/35   error=0.010124\n",
            "epoch 29/35   error=0.009746\n",
            "epoch 30/35   error=0.009376\n",
            "epoch 31/35   error=0.009062\n",
            "epoch 32/35   error=0.008717\n",
            "epoch 33/35   error=0.008439\n",
            "epoch 34/35   error=0.008186\n",
            "epoch 35/35   error=0.007959\n",
            "\n",
            "\n",
            "predicted values : \n",
            "[array([[-0.09076215, -0.03671234, -0.04502541, -0.09479849, -0.02923006,\n",
            "         0.01086008,  0.00709241,  0.98863438, -0.0309505 , -0.06138029]]), array([[ 0.20981851,  0.45037858,  0.11685538,  0.89179494, -0.03064712,\n",
            "         0.53373478, -0.32024261,  0.02065074, -0.03835505, -0.25499848]]), array([[-0.14917687,  0.99424747, -0.18113812, -0.03003979, -0.0037195 ,\n",
            "        -0.13242388, -0.05240741,  0.04423612, -0.00497623,  0.01397948]]), array([[ 0.86990043, -0.01883387, -0.14405732, -0.00175693, -0.01275043,\n",
            "         0.00855248,  0.02527364,  0.04657113,  0.03448347,  0.05107253]]), array([[ 0.14132259, -0.18319377, -0.02585718, -0.16165689,  0.96856494,\n",
            "        -0.55832451,  0.02282955,  0.17911793, -0.21544343,  0.11113056]]), array([[-0.09768557,  0.98804756, -0.11216269, -0.01052204,  0.00486531,\n",
            "         0.08283443, -0.32026937,  0.06174905, -0.04300985,  0.01022727]]), array([[ 0.06857951, -0.01092831, -0.0539233 , -0.0900087 ,  0.97139835,\n",
            "        -0.04684105, -0.00553836,  0.23598512,  0.11316853, -0.23907366]]), array([[-0.37275543, -0.02332631, -0.32993621, -0.04930308, -0.00095813,\n",
            "         0.43800027,  0.02344283,  0.21069018,  0.1154298 ,  0.88548994]]), array([[ 0.05046986,  0.28975994,  0.2068798 , -0.05241853,  0.00225994,\n",
            "         0.74313997, -0.00147504,  0.10767743, -0.06887152, -0.03636826]]), array([[-0.07174545, -0.02006986, -0.05377421, -0.07271262, -0.06160676,\n",
            "         0.00788536,  0.01526249,  0.21908649, -0.0294212 ,  0.97214934]]), array([[ 0.85504949, -0.01428148, -0.09823191,  0.00681651, -0.01236327,\n",
            "         0.06928154,  0.00781604,  0.03970696,  0.01808049,  0.03583564]]), array([[-0.17031608, -0.04912264,  0.54250055,  0.05845336,  0.08674181,\n",
            "         0.06531872,  0.0223445 ,  0.16865774, -0.02120625,  0.28915399]]), array([[-0.03821824, -0.02076269, -0.10466239, -0.12769184,  0.67396337,\n",
            "        -0.06145107, -0.00421871,  0.3295569 , -0.05596139,  0.95947357]]), array([[ 0.80525545, -0.0140208 , -0.13808789,  0.03530339, -0.01955192,\n",
            "         0.20223741,  0.00344957,  0.03884692, -0.00588094,  0.06253998]]), array([[-0.04288008,  0.95928398, -0.0816984 , -0.04761438,  0.00348775,\n",
            "         0.03421149,  0.04094199,  0.05115764, -0.03029243, -0.00945657]]), array([[ 0.10058201,  0.01719991, -0.44203452,  0.99248043, -0.12124819,\n",
            "        -0.1126307 ,  0.05530003, -0.07395149,  0.66961172, -0.29332377]]), array([[ 0.34842975, -0.01868479, -0.52455935, -0.00389439,  0.65730807,\n",
            "        -0.44047558,  0.00496459,  0.92445145,  0.11551696,  0.24012622]]), array([[-0.03931093, -0.01531443,  0.29335144, -0.08049659, -0.0381037 ,\n",
            "        -0.05361904,  0.0118913 ,  0.98110214, -0.07125487, -0.04514598]]), array([[-8.01564548e-02,  3.28644754e-02, -6.31463582e-02,\n",
            "         3.93156749e-02, -4.05411088e-02,  9.10264921e-01,\n",
            "         5.04164931e-04,  1.11944893e-01,  4.81901375e-02,\n",
            "        -7.48743752e-02]]), array([[-0.05635279, -0.01336631,  0.10619275, -0.05602251,  0.6927977 ,\n",
            "         0.08130132,  0.00817888,  0.23269598, -0.00776938,  0.50560807]]), array([[-0.1023245 , -0.01422441, -0.00769327, -0.04967224, -0.0421972 ,\n",
            "         0.14747034,  0.01460862,  0.2367088 ,  0.02762305,  0.89949554]]), array([[ 0.07053576,  0.09600035, -0.15816504, -0.1455    ,  0.32243329,\n",
            "         0.42921694,  0.5780095 ,  0.05622075,  0.30159711, -0.16030564]]), array([[-0.10002151, -0.02659037, -0.32697398,  0.05206509, -0.02214858,\n",
            "        -0.07789677,  0.96207139,  0.07239756,  0.01952051,  0.23565496]]), array([[-0.36731846, -0.00677335,  0.17189866,  0.15922986, -0.01220116,\n",
            "         0.67599929,  0.02981389,  0.13286648,  0.50982869,  0.05429837]]), array([[ 0.56637554,  0.0029916 ,  0.14412497, -0.1594414 ,  0.96804334,\n",
            "        -0.29485481, -0.00261422,  0.19702412,  0.03030413, -0.35524118]]), array([[ 0.85373871, -0.01478199, -0.06147125,  0.01080942, -0.01017028,\n",
            "        -0.15793182,  0.02470839,  0.05584401,  0.0467648 ,  0.11447053]]), array([[-0.36541808, -0.02208903, -0.13033783, -0.05204812, -0.0722999 ,\n",
            "         0.47495396, -0.01052463,  0.97375091, -0.07784201, -0.00623585]]), array([[ 5.78501784e-01, -1.40733517e-02, -6.19040144e-01,\n",
            "         1.92752928e-02,  9.71609834e-01, -6.02788708e-01,\n",
            "         6.63383963e-04,  1.93931512e-01,  1.89407360e-01,\n",
            "         2.51034733e-01]]), array([[ 0.87466307, -0.01597665, -0.16545242,  0.01858648, -0.01339199,\n",
            "        -0.03640519,  0.00786908,  0.04115284,  0.02947241,  0.10561621]]), array([[-0.02278567,  0.95927888, -0.09154285, -0.01844477, -0.00547734,\n",
            "         0.00405569,  0.0396466 ,  0.04503087, -0.01648992, -0.00843457]]), array([[-0.19442587, -0.01757665, -0.29792516,  0.9556398 , -0.03103653,\n",
            "         0.21262757,  0.0021682 ,  0.23009118,  0.01208123,  0.00789295]]), array([[-0.05513484,  0.97039173, -0.33244035, -0.15282004, -0.03228823,\n",
            "        -0.08261061, -0.2104698 , -0.21843953, -0.08777826,  0.15377745]]), array([[-0.09431345, -0.00788   ,  0.10122833,  0.98784237, -0.01027519,\n",
            "        -0.03901302, -0.15410647,  0.20931449, -0.00658197, -0.13563957]]), array([[ 0.6413351 , -0.04702834, -0.13945629, -0.18709496,  0.9633709 ,\n",
            "         0.33240992,  0.17344209,  0.07838474, -0.08828613, -0.33933553]]), array([[-0.06394409, -0.01403361, -0.05670027, -0.02034571, -0.03926912,\n",
            "         0.03406054,  0.00710645,  0.97504175, -0.01709144,  0.09696433]]), array([[ 0.07814385, -0.02226968,  0.83321478,  0.02548478, -0.00163234,\n",
            "        -0.10307142,  0.01232192,  0.08354365,  0.02886561,  0.00093676]]), array([[ 0.11235599, -0.02643171,  0.22010287,  0.14459894, -0.04744851,\n",
            "        -0.25198094,  0.00262938,  0.97815425, -0.06490582, -0.10619737]]), array([[-1.45182453e-02,  9.58985993e-01, -6.70331369e-02,\n",
            "        -2.39052071e-02, -5.02522608e-03, -1.73563113e-02,\n",
            "         4.30610510e-02,  3.66127836e-02, -4.15819424e-02,\n",
            "         7.32795535e-05]]), array([[-1.38413862e-01, -2.35025044e-02,  5.82856970e-01,\n",
            "         5.30077270e-01, -5.33743278e-02,  3.90633843e-01,\n",
            "         2.57453084e-01,  5.14607092e-02, -2.07268448e-04,\n",
            "        -1.27937437e-01]]), array([[-0.05636427,  0.95563025, -0.07211787, -0.01010958,  0.02001819,\n",
            "         0.03945527,  0.13702535,  0.02432281, -0.09422755, -0.04314239]]), array([[ 0.02533581,  0.95940575, -0.19943797,  0.24067696, -0.00966008,\n",
            "        -0.00546237,  0.03513848, -0.02165859,  0.25473035, -0.10182456]]), array([[-3.81373446e-02, -3.42545578e-02,  2.07843983e-02,\n",
            "        -9.23599798e-02, -6.09379425e-02,  4.29063796e-02,\n",
            "        -1.29250607e-04,  9.92186182e-01, -3.53029455e-02,\n",
            "        -5.69601918e-02]]), array([[ 0.040143  , -0.01619602,  0.21117058,  0.01721067,  0.43468594,\n",
            "        -0.06748565,  0.01584684,  0.22836159,  0.00045593,  0.28988842]]), array([[-0.12168731, -0.02904286,  0.85850733,  0.01078557, -0.00087256,\n",
            "         0.07180054,  0.00733343,  0.08842015,  0.04611581, -0.03696298]]), array([[-0.09751928,  0.21081538, -0.08443086,  0.01833215, -0.00568538,\n",
            "         0.24776301,  0.48581351,  0.1322406 ,  0.0550131 ,  0.21044454]]), array([[ 0.14397324, -0.02569413,  0.06236271,  0.05783418,  0.00085451,\n",
            "         0.6659866 ,  0.02832323,  0.07450162,  0.01779045,  0.10034411]]), array([[-0.39853849,  0.19821209, -0.01930019,  0.36738448, -0.00271776,\n",
            "         0.17521866,  0.37871304,  0.18235158,  0.16034443,  0.21002863]]), array([[-0.3762242 , -0.04296538,  0.69931282, -0.04385977, -0.02264696,\n",
            "         0.72679116,  0.37203214,  0.16016056, -0.02640657, -0.03720565]]), array([[-0.23629159, -0.01295766, -0.37280115,  0.02689999,  0.97122477,\n",
            "         0.04515529, -0.01285161,  0.22474188,  0.04898635,  0.11828437]]), array([[ 0.53810466, -0.03246367, -0.16915126, -0.04715873,  0.96738772,\n",
            "        -0.57761445,  0.00368635,  0.12793691,  0.05603924,  0.09160772]]), array([[-0.06296062, -0.08626341, -0.0086844 ,  0.02101472, -0.06837677,\n",
            "         0.1555559 ,  0.96505741,  0.02635959, -0.04016125,  0.02263696]]), array([[-1.52086868e-01,  5.94000050e-04,  6.72564227e-01,\n",
            "         1.28940239e-01, -1.96734387e-02,  2.61934646e-01,\n",
            "         3.09791849e-02,  1.27184812e-01, -7.28206661e-02,\n",
            "         4.39000723e-02]]), array([[-0.12598136, -0.00457044,  0.35634742, -0.08951209,  0.50596146,\n",
            "         0.34196729,  0.01921539,  0.18154051, -0.00635769,  0.27213691]]), array([[ 0.25882992, -0.03979188, -0.43535232,  0.08142917,  0.15330344,\n",
            "         0.76925753,  0.26837665, -0.03566428,  0.26887976,  0.05374846]]), array([[-0.03130871, -0.01466887,  0.82398156,  0.00300204, -0.01119522,\n",
            "        -0.02178177,  0.087741  ,  0.1030994 , -0.02032257,  0.00446193]]), array([[ 0.87116578, -0.01471896, -0.13363611,  0.00830763, -0.01298329,\n",
            "         0.0131585 ,  0.0093807 ,  0.03962078,  0.07003979,  0.03720494]]), array([[-0.10139943,  0.00112779, -0.16346265,  0.03164857,  0.9716578 ,\n",
            "         0.05571606, -0.05955925,  0.10499058,  0.04436831, -0.03385075]]), array([[ 0.02495111,  0.93011017,  0.08044555, -0.01336561, -0.02901755,\n",
            "         0.53797965, -0.48420747,  0.08735416, -0.02913985, -0.01460083]]), array([[ 0.21396429, -0.01605026, -0.26669276,  0.01897123,  0.07007599,\n",
            "        -0.22516895,  0.0203372 ,  0.22780333,  0.08711239,  0.86349525]]), array([[-0.28262667,  0.93699681,  0.24914498,  0.24738878,  0.09758447,\n",
            "         0.27035233, -0.21875263,  0.84079876,  0.20527044, -0.16562508]]), array([[-0.21613667, -0.012067  ,  0.14745757,  0.01255505, -0.01632848,\n",
            "         0.06621148, -0.00948417,  0.97740269, -0.11616677,  0.06570804]]), array([[-0.09264806, -0.02312569,  0.82184503,  0.03934217, -0.01421772,\n",
            "        -0.009124  ,  0.02782588,  0.09425134,  0.05205412,  0.05346776]]), array([[ 0.10564638,  0.03695597,  0.20950861, -0.12821704,  0.96755844,\n",
            "        -0.10907712, -0.01912264,  0.05662805, -0.08097371,  0.581025  ]]), array([[-0.14011221, -0.02371402,  0.54812827,  0.33891666, -0.0118512 ,\n",
            "         0.04558541,  0.0192079 ,  0.19791503,  0.05331001,  0.17821409]]), array([[-0.09381544, -0.01312232,  0.06903012, -0.05274867, -0.02459267,\n",
            "         0.08148259,  0.0262211 ,  0.89231632, -0.01231084,  0.34678814]]), array([[-0.49907092, -0.01631508, -0.13603615, -0.09778322,  0.79112358,\n",
            "         0.57440408, -0.01817398,  0.18935742, -0.05096829,  0.94101074]]), array([[-0.23771916, -0.01557284,  0.62105453, -0.27426179, -0.01513013,\n",
            "         0.38079473,  0.2314247 , -0.0317918 ,  0.24625092, -0.12519806]]), array([[-0.19632538, -0.00237089,  0.14730553, -0.06797065,  0.96506562,\n",
            "         0.06513796, -0.00460724,  0.17069877, -0.05697544,  0.08978113]]), array([[-0.12592274, -0.02318916, -0.22371401,  0.9757536 , -0.0308634 ,\n",
            "        -0.02554006,  0.00901487,  0.1339517 ,  0.01529013,  0.02478604]]), array([[ 0.63591023, -0.01443097,  0.27316785,  0.00976862, -0.02917682,\n",
            "         0.41854918,  0.00272554,  0.0527473 , -0.09528767, -0.02582312]]), array([[-0.01965676, -0.02667006,  0.18974853, -0.13800241, -0.01844028,\n",
            "         0.01880907,  0.01930265,  0.99627753, -0.00400161, -0.13651903]]), array([[ 0.87786242, -0.01486922, -0.1205119 ,  0.00462226, -0.01148718,\n",
            "        -0.00882134,  0.00842876,  0.04013434,  0.02331806,  0.0464357 ]]), array([[-0.05645182, -0.02527774,  0.67848488,  0.95440111, -0.00251855,\n",
            "        -0.06224176, -0.02005687,  0.05144318, -0.00942198, -0.2753725 ]]), array([[-9.03257010e-02, -1.28400118e-02,  6.28161358e-02,\n",
            "         5.71540082e-04, -2.74439710e-02,  1.17968502e-01,\n",
            "         1.92474592e-02,  7.41240773e-01,  5.15196648e-02,\n",
            "         1.98590314e-01]]), array([[-0.03535314,  0.95867075, -0.08722999, -0.02757149, -0.00187077,\n",
            "         0.0069714 ,  0.0634079 ,  0.04466808, -0.05314408, -0.00577001]]), array([[ 0.01703518, -0.01342935,  0.02783294, -0.07610141, -0.03631762,\n",
            "         0.04448896, -0.01786038,  0.96821463,  0.03853492, -0.09537397]]), array([[-0.05044244, -0.02448893,  0.10314803,  0.98076523, -0.01956879,\n",
            "         0.11469587,  0.04299049,  0.07211264, -0.01998482, -0.24018831]]), array([[ 0.14997767, -0.14167712,  0.37106347,  0.08894141, -0.26768398,\n",
            "         0.05322214, -0.151464  ,  0.98799052, -0.16186115, -0.32152134]]), array([[-7.06941998e-02, -1.48483869e-02,  6.74991756e-04,\n",
            "        -1.16707626e-02, -4.35716146e-02,  4.32520913e-02,\n",
            "        -9.08033120e-03,  2.31597986e-01, -1.37132544e-03,\n",
            "         9.41044829e-01]]), array([[-0.07157754, -0.01657175, -0.05065849, -0.07574924, -0.03861997,\n",
            "         0.03324284,  0.0143231 ,  0.98580382, -0.00719685, -0.03338156]]), array([[-0.33891714, -0.02980903, -0.29840845, -0.1287226 ,  0.01075407,\n",
            "         0.33042461, -0.05689337,  0.55834365,  0.04491404,  0.96419948]]), array([[ 3.72611552e-03, -2.32998604e-02,  1.34960594e-01,\n",
            "        -1.63927995e-05, -4.95506420e-04,  8.93659472e-02,\n",
            "         9.25000342e-01,  5.74578849e-03, -8.99800932e-03,\n",
            "        -7.54303481e-03]]), array([[-0.13496963, -0.02314422,  0.84783248,  0.02375836, -0.01049988,\n",
            "         0.03224524,  0.01622081,  0.08513654,  0.0137832 ,  0.02782803]]), array([[-0.08047707, -0.05360004, -0.03976322, -0.09102379, -0.02271747,\n",
            "        -0.0717972 , -0.01422149,  0.99424293, -0.06253186,  0.06458948]]), array([[-0.29844048, -0.01806427, -0.20249455, -0.0410995 , -0.00578905,\n",
            "         0.34107394,  0.01435979,  0.1568029 ,  0.96373251,  0.04347321]]), array([[-1.35061380e-01, -1.20207506e-02,  1.19287914e-02,\n",
            "         1.29402941e-04,  9.70976875e-01, -6.04877898e-02,\n",
            "        -5.03225111e-03,  1.79827599e-01, -1.34460579e-02,\n",
            "         3.12274551e-02]]), array([[-0.0384732 , -0.01869585, -0.00197128, -0.02975303, -0.03829159,\n",
            "        -0.02354399,  0.01029914,  0.98582395, -0.02093539, -0.09268889]]), array([[-3.32225599e-01,  1.34424491e-02, -2.58686735e-02,\n",
            "         9.85558124e-01, -7.30413087e-02,  1.65979512e-01,\n",
            "        -2.49295863e-04,  3.04158072e-03, -1.06849143e-01,\n",
            "        -1.75835336e-01]]), array([[-0.22585842, -0.04144082,  0.06526916, -0.02097573, -0.02969401,\n",
            "        -0.21928076,  0.97068041,  0.09101061, -0.01678805,  0.0311202 ]]), array([[-1.15284770e-01,  9.88330090e-01, -5.34092872e-02,\n",
            "        -6.00725575e-02, -1.14514677e-04,  1.33051069e-01,\n",
            "        -3.84593308e-01,  4.54752821e-02, -4.94190043e-02,\n",
            "        -2.01002929e-02]]), array([[-0.58121098, -0.02444898, -0.25338711,  0.97305419, -0.02516186,\n",
            "         0.55224653,  0.00124231,  0.13486673, -0.02789628, -0.01752723]]), array([[ 0.00608469, -0.19002484,  0.25011045,  0.10610058,  0.13655345,\n",
            "        -0.09086123,  0.98331479, -0.13400749, -0.34298801, -0.1920909 ]]), array([[-0.16810995, -0.02216232,  0.20027261,  0.02288001,  0.02192251,\n",
            "         0.12401356,  0.0217735 ,  0.2429641 ,  0.11785007,  0.61933879]]), array([[-0.13114235, -0.0173288 , -0.19232284,  0.86632226, -0.03441661,\n",
            "         0.15256633,  0.00812333,  0.24563981,  0.02290597,  0.10659853]]), array([[-1.07096153e-01,  9.50291776e-01,  2.57458556e-02,\n",
            "        -2.49076496e-02, -6.77578545e-04, -2.47391948e-01,\n",
            "         1.50052837e-01,  6.09417996e-02,  1.03595384e-01,\n",
            "         4.10539995e-02]]), array([[-0.0642298 , -0.00812296, -0.02815135,  0.03120495, -0.01159095,\n",
            "         0.28214752,  0.01775904,  0.25882097,  0.04431176,  0.29127049]]), array([[-0.30325369,  0.58804342, -0.45596918, -0.01998628,  0.00574971,\n",
            "         0.80460119, -0.38696766,  0.53191461,  0.16328682,  0.12160658]]), array([[-0.04507187, -0.02361686,  0.02598254,  0.05125257, -0.01210328,\n",
            "         0.32724669,  0.2865376 ,  0.20968829,  0.0649031 ,  0.19955543]]), array([[ 0.78192894,  0.00748885, -0.32104394,  0.02779143,  0.02953066,\n",
            "        -0.60149301,  0.92167901,  0.01501952,  0.02562811,  0.0511036 ]]), array([[-0.60992005, -0.01682061, -0.30934893,  0.10857513, -0.04584079,\n",
            "         0.51123628,  0.01134656,  0.42064112,  0.07593935,  0.61820873]])]\n",
            "true values : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [0]*testnum\n",
        "for i in range(testnum):\n",
        "  for j in range(1):\n",
        "    for k in range(10):\n",
        "      if out[i][j][k] == max(out[i][j]):\n",
        "        pred[i] = k+1\n",
        "        print(k+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhXy4yS2nkhO",
        "outputId": "22bd06eb-7c28-4ecd-a4ec-d8a5d3ea4304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "4\n",
            "2\n",
            "1\n",
            "5\n",
            "2\n",
            "5\n",
            "10\n",
            "6\n",
            "10\n",
            "1\n",
            "3\n",
            "10\n",
            "1\n",
            "2\n",
            "4\n",
            "8\n",
            "8\n",
            "6\n",
            "5\n",
            "10\n",
            "7\n",
            "7\n",
            "6\n",
            "5\n",
            "1\n",
            "8\n",
            "5\n",
            "1\n",
            "2\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "8\n",
            "3\n",
            "8\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "8\n",
            "5\n",
            "3\n",
            "7\n",
            "6\n",
            "7\n",
            "6\n",
            "5\n",
            "5\n",
            "7\n",
            "3\n",
            "5\n",
            "6\n",
            "3\n",
            "1\n",
            "5\n",
            "2\n",
            "10\n",
            "2\n",
            "8\n",
            "3\n",
            "5\n",
            "3\n",
            "8\n",
            "10\n",
            "3\n",
            "5\n",
            "4\n",
            "1\n",
            "8\n",
            "1\n",
            "4\n",
            "8\n",
            "2\n",
            "8\n",
            "4\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "7\n",
            "3\n",
            "8\n",
            "9\n",
            "5\n",
            "8\n",
            "4\n",
            "7\n",
            "2\n",
            "4\n",
            "7\n",
            "10\n",
            "4\n",
            "2\n",
            "10\n",
            "6\n",
            "6\n",
            "7\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arr"
      ],
      "metadata": {
        "id": "QGQItCm1GIAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(testnum):\n",
        "  for j in range(10):\n",
        "    if y_test[i][j] == 1:\n",
        "      print(j+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsCE8_MBm1rR",
        "outputId": "1e83f6ed-62e6-425e-b3e7-d9ed1e616692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "3\n",
            "2\n",
            "1\n",
            "5\n",
            "2\n",
            "5\n",
            "10\n",
            "6\n",
            "10\n",
            "1\n",
            "7\n",
            "10\n",
            "1\n",
            "2\n",
            "6\n",
            "10\n",
            "8\n",
            "4\n",
            "5\n",
            "10\n",
            "7\n",
            "7\n",
            "6\n",
            "5\n",
            "1\n",
            "8\n",
            "5\n",
            "1\n",
            "2\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "8\n",
            "3\n",
            "8\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "8\n",
            "5\n",
            "3\n",
            "4\n",
            "6\n",
            "2\n",
            "3\n",
            "5\n",
            "5\n",
            "7\n",
            "4\n",
            "6\n",
            "6\n",
            "7\n",
            "1\n",
            "5\n",
            "2\n",
            "10\n",
            "6\n",
            "8\n",
            "9\n",
            "10\n",
            "4\n",
            "8\n",
            "5\n",
            "7\n",
            "5\n",
            "4\n",
            "1\n",
            "8\n",
            "1\n",
            "3\n",
            "10\n",
            "2\n",
            "8\n",
            "4\n",
            "3\n",
            "10\n",
            "8\n",
            "8\n",
            "7\n",
            "3\n",
            "8\n",
            "9\n",
            "5\n",
            "8\n",
            "4\n",
            "7\n",
            "2\n",
            "4\n",
            "7\n",
            "10\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "8\n",
            "7\n",
            "10\n"
          ]
        }
      ]
    }
  ]
}